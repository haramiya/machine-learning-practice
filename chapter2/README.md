# 教師あり学習
## ■本章の目的と概要
- 回帰問題
- クラス分類
- テキスト分類
## ■最小二乗法による線形回帰
### 定着させておきたい点
- scikitlearnを用いて線形回帰モデルを実装できる
- 予測に使用する変数が1つの線形回帰モデルは単回帰モデル、2つ以上の線形回帰モデルは重回帰モデルである
- 線形回帰モデルでは最小二乗法を用いてパラメータを学習し求める

### アルゴリズム
- 13種類の変数を使って、その地域の住宅価格の中央値を予測
- 教師あり学習の基本的なステップ
  - ライブラリのインポート
  - 訓練データとテストデータの準備
  - アルゴリズム選択と訓練データ（及び検証データ）による学習
  - テストデータで性能を確認
  - 未知のデータを回帰（または分類）　※本節では行っていない

#### 単回帰モデル
- 「住宅の平均部屋数（RM）」のみを使用する
- .ipynb参照
- 線形回帰モデルの評価「決定係数R^2」スコアが低く、いいモデルではない
#### 重回帰モデル
- .ipynb参照
- 複数の特徴量を使用している

### 線形回帰モデル使用上の注意
- 良い回帰モデル
  - 訓練データに対する性能が高く、かつ、テストデータに対する性能もほとんど低下しないモデル
-  過学習を起こさないようにする

## ■L1正則化、L2正則化による過学習の抑制
- L1正則化（Lasso）
- L2正則化（Ridge回帰）
### 定着させておきたい点
- scikitlearnを用いてL1正則化、L2正則化を実装できる
- 正則化は重みパラメータに制約をかける項を追加して過学習を抑制する
- L1正則化（Lasso）とL2正則化（Ridge回帰）の違いは重みパラメータへの制約のかけ方である
### アルゴリズムの基本的な流れ、概要
- 上記で実装した、重回帰モデルのパラメータwが訓練データに過剰にフィットしていたのを防ぐために使用
- パラメータwの値（絶対値）が大きくなりすぎないようにペナルティを与える
  - 最小二乗法で求める誤差の総和にペナルティを足し合わせることで、過学習を防ぐ
### L1正則化とL2正則化の使い分け
- 重みパラメータの一部を0にし、回帰モデルで使用される特徴量の数を制限したいかどうか
  - 「過学習を防ぎたいが、使用する特徴量のすべてが重要な場合」はL2正則化
  - 「過学習を防ぐために、使用する特徴量が減っても良い場合」はL1正則化を採用するとよい
- 他にも、Elastic Netと呼ばれる機械学習モデルもある

## ■ロジスティック回帰によるクラス分類
### 定着させたい点
- scikitlearnを用いてロジスティック回帰モデルを実装できる
- ロジスティック回帰では、ロジスティック関数に線形関数を代入してモデルを作成する
- ロジスティック回帰モデルの尤度関数から計算した誤差関数の値が最小となるようなパラメータの値を求める

### 実現したい処理
- 分類する対象のデータとして、架空の会員顧客データ（過去1年分）を作成します。
- 会員顧客データはx軸が年間購入回数、y軸が平均購入単価を表す2次元データとします。
- ラベル（クラス）は会員継続顧客か退会顧客かを示すものとします。
- その年の終わりに会員継続した顧客であればラベル1を、退会した顧客であればラベル0を付けます。
- そして、訓練データをもとにロジスティック回帰モデルを学習し、未知の顧客が会員継続顧客と退会顧客のどちらに分類されるのかを予測する機械学習モデルを実装します。
  - つまり、図2.10のように2次元にマッピングされた2クラス（継続または退会）の未知データをなるべくうまく分類するような境界となるモデルを学習させることが目標になります。
